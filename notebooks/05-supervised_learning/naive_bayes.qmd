---
title: "Text classifiction with the Naive Bayes algorithm"
author: Hauke Licht
institute: University of Innsbruck
date: 2025-11-04
toc: true
toc-depth: 2
toc-location: right
toc-expand: true
execute:
  echo: true
  eval: true
  message: false
  warning: false
  fig-height: 2.8
  fig-width: 4.5
code-annotations: select
format:
  html:
    embed-resources: true
    self-contained: true
    anchor-sections: true
    smooth-scroll: true
---

```{r setup}
#| echo: false
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
options(scipen = 1, digits = 4) #set to two decimal 
```

# Setup

## Load required libraries

```{r libraries}
library(readr)
library(quanteda)
library(quanteda.textmodels)
library(dplyr)
library(ggplot2)
```

```{r}
#| echo: false 
library(ggplot2)
set_theme(
  theme_minimal() + 
    theme(
      theme(
        panel.background = element_rect(fill = "transparent", color = NA_character_),
        # panel.grid.major = element_blank(), # get rid of major grid
        # panel.grid.minor = element_blank(), # get rid of minor grid
        plot.background = element_rect(fill = "transparent", color = NA_character_),
        legend.background = element_rect(fill = "transparent"),
        legend.box.background = element_rect(fill = "transparent"),
        legend.key = element_rect(fill = "transparent")
    )
  )
)
```

## Load the data

We use data from the paper:

> Barberá P, Boydstun AE, Linn S, McMahon R, Nagler J. 
> "Automated Text Classification of News Articles: A Practical Guide." 
> _Political Analysis_. 2021;29(1):19-42. 
> doi:[10.1017/pan.2020.8](https://doi.org/10.1017/pan.2020.8)

This data records news articles categorized into two topic categories: "economy" and "other":

```{r data}
# NOTE: assuming you run this file in th context of our courses R project
fp <- file.path("data", "labeled", "barbera_automated_2021", "barbera_automated_2021-econ_topic.csv")

# load the CSV file
df <- read_csv(fp)

# NOTE: the label categories are "yes" (about economy) and "no" (not about economy)
df$label <- factor(df$label, c("yes", "no"), c("economy", "other"))
```

### split into train and test set

We split into two sets here:
 
- **_train_ set**: used to induce the dictionary terms using the fightin' words approach
- **_test_ set**: used to evaluate the induced dictionary terms
  
```{r create_splits_metadata}
# NOTE: set random number generation seed for reproducibility
set.seed(1234) 

# randomly assign 20% of documents to test split
df$metadata__split <- sample(
  size = nrow(df), 
  x = c("train", "test"), 
  prob = c(0.8, 0.2),
  replace = TRUE 
)
```

Random sampling maintains the overal label class distribution across splits:

```{r}
df |> 
  with(table( metadata__split, label)) |> 
  prop.table(1) |>
  round(3)
```

### Create documnent-feature matrices

We want to perform text classification.
So first, we need to convert unstructured texts into document-term matrices applying 
standard bag-of-words preprocessing.

::: {.callout-note title="Preprocessing"}

It is important to apply the same pre-processing steps to the train and test data.

:::

Since our goal is to recognize whether a news article is about the economic topic, we pre-process the texts as follows:

1. We keep symbols and numbers but remove punctuation to reduce noise.
  Typically, we'd remove all. 
  But our goal is to identify articles about the economy.
  Numbers (e.g., "inflation rose by 2%") and symbols (e.g., "$100") are likely to be informative features for this task.
2. We lowercase all tokens to reduce sparsity.
   This might cause some problems (e.g., cannot distinguish "US" from "us" or "FED" from "fed"), but overall it helps to reduce sparsity.
3. We create $n$-grams (unigrams, bigrams, trigrams) to capture multi-word expressions that might be informative (e.g., "stock market", "interest rate").
4. We lemmatize the tokens to reduce sparsity (e.g., "running", "ran", "runs" &rArr; "run").
5. We remove stopwords to focus on content words.


```{r dtm}
#| cache: true
dtm <- df |> 
  corpus(text_field = "text", docid_field = "uid") |> 
  tokens(remove_symbols=FALSE, remove_numbers=FALSE) |> 
  tokens_tolower() |> 
  tokens_ngrams(n = 1:3) |> 
  # lemmatize (based on  https://stackoverflow.com/a/62330539)
  tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) |> 
  tokens_remove(stopwords("en")) |>
  dfm()
```

```{r}
#| cache: true
#| eval: false
#| include: false
# NOTE: you can try this more complex alternative if you want
# here, I further reduce sparsity by replacing numbers in texts with a placeholder `NUM` (as far as possible)
toks <- df |> 
  corpus(text_field = "text", docid_field = "uid") |> 
  tokens(remove_symbols=FALSE, remove_numbers=FALSE, remove_punct=TRUE, xptr = TRUE) |> 
  tokens_tolower()

feats <- unique(unlist(toks))
  
num_features <- feats[grep("\\d", feats)]
num_features_hash <- data.frame(token = num_features)
num_features_hash$hash <- num_features_hash$token
num_features_hash$hash <- gsub("^(\\d*([.,;]\\d*)*)+$", "NUM", num_features_hash$hash)
num_features_hash$hash <- gsub("^\\d+-", "", num_features_hash$hash)

dtm <- toks |>
  tokens_replace(pattern = num_features_hash$token, replacement = num_features_hash$hash) |> 
  tokens_ngrams(n = 1:2) |> 
  tokens_replace(pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma) |> 
  tokens_remove(stopwords("en")) |>
  tokens(remove_punct=TRUE) |> 
  dfm()

dim(dtm)
```



In the next step, we separate the train and test data.
We then apply all data-driven pre-processing only to the test set.
This ensures that we can always apply the classifier also to data not available when preparing training.

```{r create_splits}
# get the train split data and trim it
dtm_train <- dtm |>
  dfm_subset(docvars(dtm, "metadata__split") == "train") |>
  dfm_trim(
    min_termfreq = 10, termfreq_type = "count",
    max_docfreq = 0.85, docfreq_type = "prop"
  )
sparsity(dtm_train)
dim(dtm_train)
```

```{r}
# get the test split data and only keep features that are also in the train set
dtm_test <- dfm_subset(dtm, docvars(dtm, "metadata__split") == "test")
```

# Training

Training in the context of *supervised learning* refers to the process of using labeled data to update the parameters of a prediction model.
Labeled data means that for each example in the training data, we know the "true" value of the variable we want to predict.
In classification applications the variable we want to predict is the **class label**.

The goal of this process is to find model parameters that allow the model to make accurate predictions on new, unseen data.

## The Naive Bayes model

The Naive Bayes model is a probabilistic classification algorithm based on Bayes' theorem.

It is called "naive" because it makes a strong assumption about the independence of features.
Specifically, it assumes that the features (in our case: the tokens in the DFM) are **_conditionally_ independent** given the class label.
Of course, in the context of textual data, this assumption is often violated.
For example, if a document contains the term "government", it is more likely to also contain the terms "policy", "bill", "opposition", etc. than another random document.


Despite this "naive" assumption, the Naive Bayes model often performs well in text classification tasks. <!--, particularly when the feature independence assumption is approximately true.-->

### Fitting

The `qunateda.textmodels` package makes it super easy to fit a naive bayes model with the `textmodel_nb()` function:

```{r model}
nb_model <- textmodel_nb(
  x = dtm_train, # the document feature matrix with trainining documents text representations
  y = docvars(dtm_train, "label"), # the vector indicating training documents' true labels
  distribution = "Bernoulli" # set this to binarize feature counts (otherwise "multinomial")
)
```

The relevant information is in the `params` (parameters) element of the resulting object:


```{r}
dim(nb_model$param)
nfeat(dtm_train)
```

You can see that there are there are two parameters (rows) per "feature" (term) in the training data (columns).
These are _two_ parameters per feature because in our application we have two label classes: "economy" and "other".

A simpler way to get at at the model's parameters is to use the `coef()` function (to get a model's "coefficients"):

```{r}
# let's look at parameters for first five features
coef(nb_model)[1:5, ]
```

Simply put, these values indicate whether observing a feature (e.g., "sale") makes a document more or less likely to belong to either of the two label classes.
For example, all else equal, observing the token "sale" in a document makes it more likely to be classified into the _economy_ class.

More precisely, you can interpret these values as follows:

When fit the NB model (with `distribution = "Bernoulli"`), we model the probability that a feature (word) occurs at least once in a document given the class, $\Pr(x_j=1 \mid y=c)$.
So, the values for term "sale" indicate that this term appears in about 16.9% of articles about the economy and in about 9.2% of "other" articles.

A features' _relative importance_ is therefore determined by how different these probabilities are between classes.
This can be conveniently indicated with the log-odds ratio (LOR):


```{r}
feat_imp <- as.data.frame(coef(nb_model))
feat_imp$log_odds <- log( feat_imp$economy / feat_imp$other )
```

*Positive* LOR values: features contributes to classification into economy class:

```{r}
feat_imp |> 
  arrange(desc(log_odds)) |> 
  head(10)
```
_Negative_ LOR values: features contributes to classification into "other" class

```{r}
feat_imp |> 
  arrange(log_odds) |> 
  head(10)
```
This confirms that the term "sale" positively contributes to classification into the economic class:

```{r}
feat_imp["sale", ]
```

### Predicting

We can take the model to compute the expected class category probabilities  for an unlabeled example given the fitted model's parameters.

Prediction with quanteda textmodel objects (and other model objects) is made easy with the `predict()` function.
We simply pass 

1. the model we want to use for prediction as the first argument and 
2. the document-term matrix of documents whose class we want to predict

```{r pred class}
y_test <- predict(nb_model, newdata = dtm_test, force = TRUE)
```

::: {.callout-note}
We also set `force = TRUE` to account for the fact that the columns (features) in `dtm_test` do not or might not perfectly match those in `dtm_train` use for fitting the model. 
Setting `force = TRUE` gracefully handles this scenario.
:::

The object `y_pred` we'v created records the predicted label classes of documents in `dtm_test`:

```{r}
# get the predicted labels of the first 5 documents
y_test[1:5]
```

Overall, we get the following predicted label classes:

```{r}
table(y_test)
```

We can also use the `predict()` function to get documents' predicted class probabilities:

```{r pred prob}
y_test_probs <- predict(nb_model, newdata = dtm_test, type = "probability", force = TRUE)
```

```{r}
# get the predicted class probablities of the first 5 documents
y_test_probs[1:5, ]
```

The model seems to be overall very certain in its predictions.
In many cases one of the classes' predicted probability is close to 1.0.

```{r}
ggplot(mapping = aes(x = y_test_probs[ , 1])) + 
  geom_histogram() + 
  xlab('pred. prob. for label class "economy"')
```

# Evaluation

Evaluation in the context of supervised learning refers to the process of measuring the "performance" of a trained model.
Specifically, a model's _predictive_ performance.
_Predictive_ performance refers to how well the values a model predicts for some inputs aligns with these inputs' actual, "true" values.

In the context of **text classification**, _predictive_ performance therefore focuses on the agreement between the labels a model predicts for documents and their actual, "true" labels.

## Predict on test data

Evaluation is typically done on a so called **test set**.
This dataset contains examples (inputs + labels) that have _not_ been used for training the model.
Because these examples were not used during training, they are referred to as 
"held-out" or "unseen" examples.

Evaluating a model on held-out examples gives us an idea of how well the model generalizes to new, previously unseen data.
This is important to assess how well it will work when applied to new data.

To enable this, it is important to pre-process the test data in the same way as the training data.
The only thing we do not need to do is trimming the DFM, because we want to use the same features (terms) as were available in the training data.

```{r evaluation}
y_pred <- predict(nb_model, newdata = dtm_test, force = TRUE)
```

### The "confusion matrix"

The *confusing matrix* (or *confusion table*) is a useful tool to eyeball the performance of a classifier.
It is a table with two dimensions:

- rows: the *predicted* labels assigned by the classifier
- columns: the actual ("true") labels in the test data

In the cells, the confusing matrix counts the number of documents for each combination of predicted and actual labels

cells on the diagonal therefore show the number of correctly classified documents.
The off-diagonal cells show the number of misclassifications.

```{r confusion_matrix}
y_true <- docvars(dtm_test, "label")
cm <- table(Predicted = y_pred, Actual = y_true)
cm
```

This shows that in our test set,

- `r cm["economy", "economy"]` documents were correctly classified as "economy"
- `r cm["other",   "other"]`   documents were correctly classified as "other"
- `r cm["economy", "other"]`   documents about "other" topics were misclassified as "economy"
- `r cm["other", "economy"]`   documents about "economy" were misclassified as "other"

In binary classification, these entries are referred to as follows:

- documents were correctly classified as "economy" &rArr; *true positives* (TP)
- documents were correctly classified as "other" &rArr; *true negatives* (TN)
- documents about "other" misclassified as "economy" &rArr; *false positives* (FP)
- documents about "economy" misclassified as "other" &rArr; *false negatives* (FN)

```{r}
tp <- cm["economy", "economy"]
tn <- cm["other", "other"]
fp <- cm["economy", "other"]
fn <- cm["other", "economy"]

cat(
  "True Positives  (economy ✅): ", tp, "\n",
  "True Negatives  (other ✅):   ", tn, "\n",
  "False Positives (economy ❌): ", fp, "\n",
  "False Negatives (other ❌):   ", fn, "\n",
  sep = ""
)
```


## Compute evaluation metrics

We can now compute some common evaluation metrics based on the confusion matrix.

**Accuracy**: The proportion of correctly classified documents out of all documents.
That is, we only need to 

1. add the number of true positives and true negatives and 
2. divide them by the total number of documents.

```{r accuracy}
accuracy <- (tp + tn) / sum(cm)
cat("Accuracy: ", round(accuracy, 3))
```

**Precision**: The proportion of correctly classified positive documents (i.e., economy) out of all documents classified as positive.
That is, we need to

1. get the number of cases the classifier has labeled as "economy" (inluding correct and incorrect classifications) and
2. see how many of these are actually correct.

```{r precision}
cat("No. of predicted 'economy' cases: ", tp + fp, "\n")
cat("No. of these that are correct:    ", tp, "\n")

precision <- tp / (tp + fp)
cat("↳ Precision: ", round(precision, 3))
```

**Recall**: The proportion of actual "economy" documents (i.e., economy) that were correctly classified.
  That is, we need to
  1. get the number of actual "economy" documents (including correct and incorrect classifications) and
  2. see how many of these were correctly identified by the classifier.

```{r recall}
cat("No. of actual 'economy' cases: ", tp + fn, "\n")
cat("No. of these that are correct:  ", tp, "\n")

recall <- tp / (tp + fn)
cat("↳ Recall: ", round(recall, 3))
```

Recall and precision give us important information about whether a model tends to over- or under-estimate how prevalent the positive label class (here, "economy") is:

- **_low_ precision** means that many of the examples predicted to be a positive instance (here, about the economic topic) are actual false predictions $\Rightarrow$ the model tends to _overestimate_ the prevalence of the positive label class
- **_low_ recall** means that many of the acutal positive examples are misclassified by the model $\Rightarrow$ the model tends to _underestimate_ the prevalence of the positive label class
- viewed together
  - the model overestimates prevalence if precision much higher than recall (i.e., precision/recall > 1)
  - the model underestimates prevalence if precision is much lower than recall (i.e., precision/recall < 1)
  - the model just performs poorly if both precision and recall are low
  
In our precision/recall is `r precision/recall`, suggesting that the risk of overestimating is not negligible.

Finally, there is a metric that summarizes the information provided by the precision and recall into one value: the **F1 score**:

```{r f1_score}
f1_score <- 2 * (precision * recall) / (precision + recall)
f1_score
```

## Assessing bias

Bias means that measurement error is systematic.
Systematic how?
Well, this depends on what factors the researcher considers when assessing bias.
Typically, we can use document level metadata that groups documents into different subgroups.
In comparative political research, typical factors are 

- author, speaker, or publisher (e.g., party or newspaper);
- author/speaker characteristics (e.g., ideological leaning, gender, etc.);
- and time (e.g., decade, year, month, or time of day).

One way of assessing **assessing bias** is to quantify a model's predictive performance by subgroup.
In our data set, the only interesting metadata is **article's publishing date**.
So we take this information to group documents by the decade in which they were published:

```{r}
# add year and decade indicators to doc vars
dtm_test@docvars <- dtm_test@docvars |> 
  mutate(
    metadata__year = as.integer(format(metadata__date, "%Y")),
    metadata__decade = round(metadata__year, -1)
  )

table(docvars(dtm_test, "metadata__decade"))
```

::: {.callout-note}

In the cell above, I use the `@` to directly access the data frame of document variables attached to `dtm_test`.
This is equivalent to `docvars(dtm_testm)` but gives me a little bit more flexibility to dynamically create  new document variables.
:::

To compute predictive performance by subgroup we,

1. define two helper functions for computing precision and recall
2. determine the distinct decade values and assign them to `decades`
3. create a matrix `mets` that will record the decade-wise precision and recall estimates
4. iterate over decade values and compute the precision and recall in each subset of the test set
5. compute the precision--recall ratio that indicates over-/under-estimation risk

```{r}
compute_precision <- function(true, pred) {
  cm <- table(pred, true)
  tp <- cm[1, 1]
  fp <- cm[1, 2]
  return( tp / (tp + fp) )
}

compute_recall <- function(true, pred) {
  cm <- table(pred, true)
  tp <- cm[1, 1]
  fn <- cm[2, 1]
  return( tp / (tp + fn) )
}

decades <- sort(unique(docvars(dtm_test, "metadata__decade")))

mets <- matrix(NA_real_, nrow = length(decades), ncol = 2)
rownames(mets) <- decades
colnames(mets) <- c("recall", "precision")

for (dec in decades) {
  sub_dtm = dfm_subset(dtm_test, docvars(dtm_test, "metadata__decade") == dec)
  y_true <- docvars(sub_dtm, "label")
  y_pred <- suppressWarnings(predict(nb_model, sub_dtm, force = TRUE))
  mets[as.character(dec), 1] <- compute_recall(y_true, y_pred)
  mets[as.character(dec), 2] <- compute_precision(y_true, y_pred)
}

mets <- as.data.frame(mets)

mets$pr_ratio <- mets$precision / mets$recall

mets$decade <- row.names(mets)

mets
```

This shows that the classifier tends to overestimate the prevalence of the economic topic in all decades _but_ to varying degrees.
For example, in documents published in the 2000s, the ration of precision to recall indicates very little overstimation risk whereas in the 2010s the risk is 9 times higher.

```{r}
mets |>
  ggplot(mapping = aes(y = pr_ratio, x = decade)) + 
    geom_col(alpha = 0.9) + 
    ylim(0, 2) +
    labs(y = "ratio: precision / recall", x = "decade")
```
